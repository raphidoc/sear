---
title: "sear_dev"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{sear_dev}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette intend to document the development and the functionality of the
`run_app()` function. This function is the top level function that call the
shiny app designed to process the data from the watercraft HOCR, BB3, SeaOWL, CTD and SUNA.

```{r setup}
# golem::document_and_reload()

library(DBI)

# library(sear)
library(dplyr)
library(lubridate)
library(purrr)
library(plotly)
library(readr)
library(stringr)
library(tidyr)
library(suncalc)
```

```{r}
TxtFile <- system.file("data-raw", "reduced", "DATA_20220706_153838.txt", package = "sear", mustWork = T)

MTELog <- sear:::read_mtelog(TxtFile)

Apla <- sear:::read_apla(MTELog)

MainLog <- Apla %>%
  rename(lon = lon_dd, lat = lat_dd)


SBE19 <- sear:::read_sbe19(MTELog)

BinFile <- system.file("data-raw", "MTE", "DATA_20220706_153838.bin", package = "sear", mustWork = T)

MTEHOCR <- sear:::read_mte_hocr(BinFile)

RawFile <- system.file("data-raw", "SatView", "2019-236-130913.raw", package = "sear", mustWork = T)

SatViewHOCR <- sear:::read_satview_hocr(RawFile)

Apladate <- unique(date(Apla$date_time))

HOCRtimeIndex <- as.POSIXct(unlist(purrr::map(
  .x = HOCR,
  ~ clock::date_time_parse(
    paste0(
      Apladate, " ", hms::as_hms(.x$gpstime / 1000)
    ),
    zone = "UTC"
  )
)), tz = "UTC")
```

# Foreword

The idea behind this application software originated from a conversation with myself during the first year of my master thesis.

> Do we have a way to manage and process data in a usable and consistent way ? No. Ok so it's On !

* [https://en.wikipedia.org/wiki/Usability](Usability) :

> Capacity of a system to provide a condition for its users to perform the tasks safely, effectively, and efficiently while enjoying the experience.

* [https://en.wikipedia.org/wiki/Consistency_(disambiguation)](Consistency) :

> a consistent theory is one that does not lead to a logical contradiction.

Software as a Service offer a solution to have a common architecture and code base to be shared among people while suiting their specific need. Managing and processing different data type of various format coming from different instruments require a trade off between the rigidity of a consistent architecture and the flexibility required to solve various problems. This trade off is best achieved today by web-based application. That's why *sear* is such an application.

This software is originally developed and tailored for the Sea-Doo/MTE data logger, but it could provide a common ground in ocean optics science data management and processing. If your willing for us (Ocean Optic Researcher) to meet halfway somewhere, it could be here. Future issues, necessary decision and trade of could bring this community closer and help us move toward a more collaborative future (e.g. [https://science.nasa.gov/open-science/transform-to-open-science](NASA TOPS program)). We may be able to leave behind the need to reinvent the wheel at each new generation of young scientist.
(Also it's often by reinventing the wheel that you understand how it work and that you can appropriate some principle to develop new idea)

The name *sear* come from a play of word with "sea R" and "seer", meaning having the power of vision (future or whatever). I discover later that in English sear is a verb meaning:

* To burn something with sudden intense heat
* An image, emotion, or feeling having a sudden and strong effect on someone

# Writing the sear documentation

The sear documentation is written following the principles of [https://diataxis.fr/](diataxis).
It revolves around four kind of documentation:

1. Tutorial
  Story that take a beginner by the hand to reach a meaningful goal to create engagement and provide a learning experience. Like a cooking lesson.
2. How-to
  Step-by-step guide that assume a minimal knowledge of the reader and give him instruction to go from a to b. Like a recipe.
3. Reference
  Concise description of the software. In this case it is generated with `roxygen2`.
4. Explanation
  Discussion about the software, the architecture and the choices made. Mainly this document.

# Dev Test and Demo dataset

Because of continuous acquisition, the HOCR SeaDoo data recorded by the MTE data logger is quite large (46.6M) and take time to parse from binary (~5.20 min giving 1 min to load 8.9M with a python virtualenv).
To parse and load the full AlgaeWise dataset (195.8M) it take 22 min just to parse HOCR, the other time consuming operation is to create the data synthesis for each instrument (see section [MainLog]), for now MainLog is recreated from scratch each time data is added when loading the final .bin and .txt files, it takes XXX (too long) minutes to compute.
To develop, test and demonstrate the software capabilities, a small data set is needed. As the most complete to date (2022), with the full data logger suit (except SUNA) plus BioSonic and r2Sonic, and spanning over multiple days, the AlgaeWise(2022) data offer the best option.

## Reduce raw data

To reduce the raw .bin and .txt data we start by identifying with the help of the app, on the full parsed data set, the time interval of the observation that we would like to keep.
We also identifies the position of the corresponding HOCR binary packets on the time index.

```{r}
# full <- "/D/Data/sear_test_data/full/"
# reduced <- "/D/Data/sear_test_data/reduced/"
```

TODO: Describe the selected data and explain it's utility (the logic of the selection)

* Each different day to showcase the filtering and selection capabilities
* Shallow (~1.5m) and deep water (~20m)
* Full range of boat solar azimuth

```{r eval=FALSE, include=TRUE}
# Cloudy and wave that day
D02 <- list(
  strftime(seq(ymd_hms("2022-07-02 16:31:00"), ymd_hms("2022-07-02 16:32:00"), by = 1), format = "%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-02 19:08:23"), ymd_hms("2022-07-02 19:08:45"), by = 1), format = "%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-02 19:32:51"), ymd_hms("2022-07-02 19:33:26"), by = 1), format = "%H:%M:%S", tz = "GMT")
)

D05 <- list(
  strftime(seq(ymd_hms("2022-07-05 11:50:27"), ymd_hms("2022-07-05 11:51:02"), by = 1), format = "%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-05 14:14:51"), ymd_hms("2022-07-05 14:21:16"), by = 1), format = "%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-05 14:40:20"), ymd_hms("2022-07-05 14:40:47"), by = 1), format = "%H:%M:%S", tz = "GMT")
)

D0615 <- list(
  strftime(seq(ymd_hms("2022-07-06 15:48:00"), ymd_hms("2022-07-06 15:55:00"), by = 1), format = "%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-06 16:54:45"), ymd_hms("2022-07-06 17:02:41"), by = 1), format = "%H:%M:%S", tz = "GMT")
)

D0618 <- list(
  strftime(seq(ymd_hms("2022-07-06 20:26:50"), ymd_hms("2022-07-06 20:29:28"), by = 1), format = "%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-06 20:48:26"), ymd_hms("2022-07-06 20:50:28"), by = 1), format = "%H:%M:%S", tz = "GMT")
)

D07 <- list(
  strftime(seq(ymd_hms("2022-07-07 19:26:04"), ymd_hms("2022-07-07 19:38:31"), by = 1), format = "%H:%M:%S", tz = "GMT")
)

D08 <- list(
  strftime(seq(ymd_hms("2022-07-08 19:10:42"), ymd_hms("2022-07-08 19:14:52"), by = 1), format = "%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-08 19:36:28"), ymd_hms("2022-07-08 19:39:19"), by = 1), format = "%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-08 20:47:42"), ymd_hms("2022-07-08 20:50:10"), by = 1), format = "%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-08 20:53:44"), ymd_hms("2022-07-08 20:56:34"), by = 1), format = "%H:%M:%S", tz = "GMT")
)
```

### Reducing txt files. Easy Peasy Lemon Squeezy !

```{r eval=FALSE, include=TRUE}
trim_MTELog <- function(If, Of, timeSeq, n_head) {
  Head <- readr::read_lines(If, n_max = n_head)
  readr::write_lines(Head, Of)

  Data <- readr::read_lines(If, skip = n_head)

  Pat <- str_c(unlist(timeSeq), collapse = "|")
  Tind <- str_detect(Data, Pat)
  any(Tind)
  readr::write_lines(Data[Tind], Of, append = T)
}

If <- file.path(full, "DATA_20220702_154124.txt")
Of <- file.path(reduced, "DATA_20220702_154124.txt")
trim_MTELog(If, Of, D02, n_head = 6)

If <- file.path(full, "DATA_20220705_105732.txt")
Of <- file.path(reduced, "DATA_20220705_105732.txt")
trim_MTELog(If, Of, D05, n_head = 6)

If <- file.path(full, "DATA_20220706_153838.txt")
Of <- file.path(reduced, "DATA_20220706_153838.txt")
trim_MTELog(If, Of, D0615, n_head = 6)

If <- file.path(full, "DATA_20220706_183905.txt")
Of <- file.path(reduced, "DATA_20220706_183905.txt")
trim_MTELog(If, Of, D0618, n_head = 6)


If <- file.path(full, "DATA_20220707_181725.txt")
Of <- file.path(reduced, "DATA_20220707_181725.txt")
trim_MTELog(If, Of, D07, n_head = 6)

If <- file.path(full, "DATA_20220708_165254.txt")
Of <- file.path(reduced, "DATA_20220708_165254.txt")
trim_MTELog(If, Of, D08, n_head = 6)


If <- file.path(full, "SBES_20220701_20220708.csv")
Of <- file.path(reduced, "SBES_20220701_20220708.csv")
trim_MTELog(If, Of, list(D02, D05, D0615, D0618, D07, D08), n_head = 1)
```

### Reducing HOCR binary file ... Raaaaagh ...

Count Number of byte by file, divide by number of byte per packets, find position of packets to extract with the time index.

I just realize now that the header size is equal to any packet size (512 bytes). Now it make a lot of sense why there was so much empty space after the header !
The fixed size memory entry is 512 bytes. I would like to learn more about that !
Wah ! The `dd` command use 512 as a default number of bytes for the `-bs` argument !
Whats behind this number !?

```{r eval=FALSE, include=TRUE}
trim_hocr <- function(If, Of, timeIndex, timeInt) {
  # Copy header
  system2("dd", paste0("if=", If, " bs=512", " count=", 1, " >>", Of))

  for (Tint in timeInt) {
    print(Tint)

    ind <- which(purrr::map_lgl(.x = timeIndex, ~ .x %within% Tint))
    BytesStart <- min(ind) #* 512
    BytesEnd <- max(ind) #* 512
    Skip <- BytesStart - 1
    Count <- BytesEnd - BytesStart

    print(Skip)
    print(Count)

    # Append matching packets
    system2("dd", paste0("if=", If, " bs=512", " skip=", Skip, " count=", Count, " >>", Of))
  }
}

# MTEHeader <- 512
MTEBytes <- 512

######### F02

# use  < to remove the filename
F02bin <- file.path(full, "DATA_20220702_154124.bin")
F02out <- file.path(reduced, "DATA_20220702_154124.bin")
F02Nbytes <- as.numeric(system2("wc", paste0("-c < ", F02bin), stdout = T))

F02Nbytes / MTEBytes
# 84409

hocr_raw <- sear:::read_hocr(F02bin)
# Invalid HOCR packets detected and removed: 17
# 84409 - 17 - 1 = 84391 yeah !

timeIndex <- purrr::map(
  .x = hocr_raw,
  ~ clock::date_time_parse(
    paste0(ymd("20220702"), " ", hms::as_hms(.x$gpstime / 1000)),
    zone = "UTC"
  )
)

timeInt <- list(
  interval(ymd_hms("2022-07-02 16:31:00"), ymd_hms("2022-07-02 16:32:00")),
  interval(ymd_hms("2022-07-02 19:08:23"), ymd_hms("2022-07-02 19:08:45")),
  interval(ymd_hms("2022-07-02 19:32:51"), ymd_hms("2022-07-02 19:33:26"))
)

trim_hocr(If = F02bin, Of = F02out, timeIndex, timeInt)

######### F05

# use  < to remove the filename
F05bin <- file.path(full, "DATA_20220705_105732.bin")
F05out <- file.path(reduced, "DATA_20220705_105732.bin")

hocr_raw <- sear:::read_hocr(F05bin)
# Invalid HOCR packets detected and removed: 17
# 84409 - 17 - 1 = 84391 yeah !

timeIndex <- purrr::map(
  .x = hocr_raw,
  ~ clock::date_time_parse(
    paste0(ymd("20220705"), " ", hms::as_hms(.x$gpstime / 1000)),
    zone = "UTC"
  )
)

timeInt <- list(
  interval(ymd_hms("2022-07-05 11:50:27"), ymd_hms("2022-07-05 11:51:02")),
  interval(ymd_hms("2022-07-05 14:14:51"), ymd_hms("2022-07-05 14:21:16")),
  interval(ymd_hms("2022-07-05 14:40:20"), ymd_hms("2022-07-05 14:40:47"))
)

trim_hocr(If = F05bin, Of = F05out, timeIndex, timeInt)

######### F0615

# use  < to remove the filename
F0615bin <- file.path(full, "DATA_20220706_153838.bin")
F0615out <- file.path(reduced, "DATA_20220706_153838.bin")

hocr_raw <- sear:::read_hocr(F0615bin)
# Invalid HOCR packets detected and removed: 17
# 84409 - 17 - 1 = 84391 yeah !

timeIndex <- purrr::map(
  .x = hocr_raw,
  ~ clock::date_time_parse(
    paste0(ymd("20220706"), " ", hms::as_hms(.x$gpstime / 1000)),
    zone = "UTC"
  )
)

timeInt <- list(
  interval(ymd_hms("2022-07-06 15:48:00"), ymd_hms("2022-07-06 15:55:00")),
  interval(ymd_hms("2022-07-06 16:54:45"), ymd_hms("2022-07-06 17:02:41"))
)

trim_hocr(If = F0615bin, Of = F0615out, timeIndex, timeInt)

######### F0618

# use  < to remove the filename
F0618bin <- file.path(full, "DATA_20220706_183905.bin")
F0618out <- file.path(reduced, "DATA_20220706_183905.bin")

hocr_raw <- sear:::read_hocr(F0618bin)
# Invalid HOCR packets detected and removed: 17
# 84409 - 17 - 1 = 84391 yeah !

timeIndex <- purrr::map(
  .x = hocr_raw,
  ~ clock::date_time_parse(
    paste0(ymd("20220706"), " ", hms::as_hms(.x$gpstime / 1000)),
    zone = "UTC"
  )
)

timeInt <- list(
  interval(ymd_hms("2022-07-06 20:26:50"), ymd_hms("2022-07-06 20:29:28")),
  interval(ymd_hms("2022-07-06 20:48:26"), ymd_hms("2022-07-06 20:50:28"))
)

trim_hocr(If = F0618bin, Of = F0618out, timeIndex, timeInt)

######### F07

# use  < to remove the filename
F07bin <- file.path(full, "DATA_20220707_181725.bin")
F07out <- file.path(reduced, "DATA_20220707_181725.bin")

hocr_raw <- sear:::read_hocr(F07bin)
# Invalid HOCR packets detected and removed: 17
# 84409 - 17 - 1 = 84391 yeah !

timeIndex <- purrr::map(
  .x = hocr_raw,
  ~ clock::date_time_parse(
    paste0(ymd("20220707"), " ", hms::as_hms(.x$gpstime / 1000)),
    zone = "UTC"
  )
)

timeInt <- list(
  interval(ymd_hms("2022-07-07 19:26:04"), ymd_hms("2022-07-07 19:38:31"))
)

trim_hocr(If = F07bin, Of = F07out, timeIndex, timeInt)

######### F08

# use  < to remove the filename
F08bin <- file.path(full, "DATA_20220708_165254.bin")
F08out <- file.path(reduced, "DATA_20220708_165254.bin")

hocr_raw <- sear:::read_hocr(F08bin)
# Invalid HOCR packets detected and removed: 17
# 84409 - 17 - 1 = 84391 yeah !

timeIndex <- purrr::map(
  .x = hocr_raw,
  ~ clock::date_time_parse(
    paste0(ymd("20220708"), " ", hms::as_hms(.x$gpstime / 1000)),
    zone = "UTC"
  )
)

timeInt <- list(
  interval(ymd_hms("2022-07-08 19:10:42"), ymd_hms("2022-07-08 19:14:52")),
  interval(ymd_hms("2022-07-08 19:36:28"), ymd_hms("2022-07-08 19:39:19")),
  interval(ymd_hms("2022-07-08 20:47:42"), ymd_hms("2022-07-08 20:50:10")),
  interval(ymd_hms("2022-07-08 20:53:44"), ymd_hms("2022-07-08 20:56:34"))
)

trim_hocr(If = F08bin, Of = F08out, timeIndex, timeInt)
```

# UI

## Style

The UI use the 'norwegian blue' (The dead parrot) #4f89b0, rgb(79, 137, 176) as theme color

## Project workflow

The first step to work with `sear` is to select a project folder. a project folder is identified by the presence of a `.searproj` file, this file additionally contain relevant metadata such as the project path, date of creation and last update. Other useful information could be added.
If the selected folder does not contains `.searproj` file, one is created. The root of the project is set as the folder in which this file live.

The app will also look for folder and data files to re-initiate the app state as it was in the last processing save.

## Load Data

### MTE data logger

If no data where previously loaded, you need to provide some to the app. For now, only data from MTE datalogger can be parsed to populate the app. The raw MTE files (.bin and .txt) are saved in `.sear/data/raw`. On load, the app will parse and save the resulting applanix data, HOCR packet, HOCR dark packet, HOCR time index, SBE19, SeaOwl and BBFL2 in `.sear/data/parsed`. This allow to save considerable time when re-opening the app as the bulk files are quite large and parsing them take time (~2/3 min for .bin = 46 MB).
If parsed data is already present, it will be read at the project selection.

When starting the MTE DataLogger wait to recieve the NMEA \$UTC string to synchronize itself with that time before starting logging. The time difference between the DataLogger time and \$xxGGA is about ~10 milliseconds.
There appear to be no time difference between the DataLogger and $UTC at the millisecond precision.

The DataLogger record the NMEA strings at 1Hz (probably output rate of the Appalnix).
the time stamp of the DataLogger is given with millisecond precision.
We can create a MainLog whose time will be use as coordinate for every measurments.
Considering the output rate of the NMEA strings, rounding the time at the second seem appropriate.

Rounding might be enough, but strict checking seems a good idea. The function `unique_date_time_second` does that.

#### HOCR specific files

The HOCR time index optimize processing time for the selection of HOCR packet within a time interval.
The HOCR dark is a list of shutter dark values interpolated to the minute to apply dark correction.

### BioSonic MX

Just select the .csv file processed with the dedicated software (biosonic Visual Aquatic).

TODO: Check the reference for the elevation values outputted by this software
TODO: Create a processing pipline from the MX raw data

### Hydroball

TODO: Create a processing pipeline for the hydroball WISE-Man 2019 setup.

## Selection display

### MainLog

When parsed data is loaded in the app, the `MainLog` is populated from the positioning system of the instruments package. For now only the SeaDoo Applanix can populate the `MainLog`. During the `MainLog` population (event dependent on `DataParsed`), a data synthesis is calculated from the `date_time` values of each instrument. As this per-row operation takes time, `MainLog` is saved under `.sear/data/parsed/main_log_[:digit:]{8}_[:digit:]{6}.csv`.

TODO: could be improved by computing only for the new piece of Apla data that is added (but still on the complete time index of instrument) or could filter the time index of instrument by the interval of min and max time of the new piece of apla !

NEW: The `half_time` algorithm try to improve computation time.
It slice the time vector by half on each iteration and match to any near
4 or less remaining date_time.

```{r}
x <- seq(as_date_time("2014-01-13 00:00:01"), as_date_time("2014-01-13 23:59:59"), by = 1)
y <- seq(as_date_time("2014-01-13 00:00:01"), as_date_time("2014-01-13 23:59:59"), by = 0.1)

data_synthesis_any_near <- function(x, y, tol = 3) {
  purrr::map_lgl(
    .x = x,
    ~ any(near(as.numeric(.x), as.numeric(y), tol = tol))
  )
}



data_synthesis_half_time <- function(x, y, tol = 3) {
  half_life <- function(x, y, tol = tol) {
    condition <- T
    y3 <- y

    while (condition) {
      y2 <- y3[length(y3) / 2]

      if (x <= y2) {
        y3 <- y3[1:(length(y3) / 2)]
      } else {
        y3 <- y3[(length(y3) / 2 + 1):length(y3)]
      }

      condition <- length(y3) <= 2
    }

    return(any(near(x, y3, tol = tol)))
  }

  purrr::map_lgl(
    .x = x,
    ~ half_life(as.numeric(.x), as.numeric(y), tol = tol)
  )
}

microbenchmark::microbenchmark(
  times = 1,
  AnyNear = data_synthesis_any_near(x, y, tol = 3),
  HalfLife = data_synthesis_half_time(x, y, tol = 3)
)
```

From this `MainLog`, the selection display column in the dashboard body is populated. This selection display is composed of a map which expose the discrete (aggregated to second) data point, of several filters (speed, time, need to add pitch and roll when available) and of a polar plot representing the solar azimuth relative to the boat heading of points selected on the map.

The points selected on the plotly map (lasso or box) will be processed when the `Process to L1b` button is clicked.

### NMEA string

TODO: List the required and optional NMEA string that sear will use.

**Mandatory**:
*GPGGA
*GPVTG

### date_time filtering

date_time classe is huge to store as is. As number of second since origin it is up to 100 times smaller.



**MANDATORY**

### Speed

**MANDATORY**: can be directly taken from the appropriate (GPVTG) NMEA string, or could be computed from GPGGA ?

### Azimuth polar plot

**MANDATORY**: boat azimuth can be taken from the appropriate VTG NMEA string, or could be computed from GGA from two lon lat points ?

First compute the sun azimuth.

```{r}
# Apla <- Apla %>% rename(date = date_time, lat = lat_dd, lon = lon_dd)
#
# # Solar altitude above the horizon in radian and azimuth in radian from south to west
# PosSol <- suncalc::getSunlightPosition(data = Apla, keep = c("altitude", "azimuth"))
#
# Apla <- left_join(Apla, PosSol, by = c("date", "lat", "lon")) %>%
#   rename(date_time = date, lat_dd = lat, lon_dd = lon, sol_azi = azimuth, SolAlt = altitude) %>%
#   mutate(sol_azi = sol_azi * 180/pi + 180, # convert rad to degree and shift to north reference
#          SolAlt = SolAlt * 180/pi)
```

Take the boat azimuth (relative to true north) from the applanix, then subtract one by the other to find boat azimuth from the sun.

```{r}
Station1PaP <- interval(ymd_hms("2021-11-11 17:50:31"), ymd_hms("2021-11-11 17:50:40"))

temp <- Apla %>% filter(date_time %within% Station1PaP)

# Apla <- Apla %>% mutate(
#   boat_raa = sol_azi-course_tn
# )

m <- list(
  l = 40,
  r = 30,
  b = 30,
  t = 30,
  pad = 0
)

temp %>%
  plot_ly(
    type = "scatterpolar",
    r = ~speed_kn,
    Theta = ~boat_raa,
    mode = "markers"
  ) %>%
  layout(
    autosize = F,
    margin = m,
    polar = list(
      angularaxis = list(
        # rotation = ~mean(course_tn, na.rm = T)+90,
        direction = "clockwise"
      )
    )
  )
```

### pitch and roll

*Mandatory*

pitch and roll are taken from the PASHR NMEA string of the Aplanix.
The PASHR string appear to be recorded at 1 Hz, therfore fixing the time limit for any further analysis.

```{r}
SQLDemo <- DBI::dbConnect(
  RSQLite::SQLite(),
  system.file("data-raw", "demo_sear.sqlite", package = "sear", mustWork = T)
)
```

```{sql, connection=SQLDemo, output.var = "SQLDemo"}
SELECT metadata_l1b.date_time, metadata_l1b.lon, metadata_l1b.lat, Speed, sol_zen, sol_azi, boat_raa, roll, pitch, heading, heave, instrument, sn, id, type, wavelength, channel, metadata_l1b.uuid_l2 FROM metadata_l1b
LEFT JOIN hocr_l1b ON metadata_l1b.uuid_l2 = hocr_l1b.uuid_l2
```

```{r}
# convert to cartesian

library(plot3D)
library(viridis)

test <- SQLDemo %>%
  filter(
    pitch < 10,
    roll < 10,
    !is.na(channel),
    date(date_time) == "2022-07-08", # uuid_l2 == "b7b6ab30-63e4-11ee-b767-3cf011fcb4b7",
    type == "LU", wavelength == 580.44
  ) %>%
  # filter(type == "LU" & wavelength == 580.44 & uuid_l2 == "b7b6ab30-63e4-11ee-b767-3cf011fcb4b7") %>%
  mutate(
    # pitch = -pitch, # negate pitch nd roll to swhicth between Apllanix coordinate system and Plotly
    # roll = -roll,
    # Convert sun azimuth and zenith angles to Cartesian coordinates
    Solx = sin((90 - sol_zen) * pi / 180) * cos(sol_azi * pi / 180),
    Soly = sin((90 - sol_zen) * pi / 180) * sin(sol_azi * pi / 180),
    Solz = cos((90 - sol_zen) * pi / 180),
    x1 = sin(heading * pi / 180) * cos(pitch * pi / 180),
    y1 = sin(heading * pi / 180) * sin(pitch * pi / 180),
    z1 = sin(pitch * pi / 180),
    Phi = roll * pi / 180,
    Theta = pitch * pi / 180,
    Psi = heading * pi / 180
  )
```

```{r}
# create_cartesian_sphere_with_angles <- function(radius, latitude_divisions, longitude_divisions) {
#   vertices <- matrix(0, nrow = (latitude_divisions + 1) * (longitude_divisions + 1), ncol = 5)
#   indices <- matrix(0, nrow = latitude_divisions * longitude_divisions * 2, ncol = 3)
#
#   vertex_count <- 1
#   index_count <- 1
#
#   for (lat in 0:latitude_divisions) {
#     Theta <- lat * pi / latitude_divisions
#     sin_Theta <- sin(Theta)
#     cos_Theta <- cos(Theta)
#
#     for (lon in 0:longitude_divisions) {
#       phi <- lon * 2 * pi / longitude_divisions
#       sin_phi <- sin(phi)
#       cos_phi <- cos(phi)
#
#       x <- radius * sin_Theta * cos_phi
#       y <- radius * sin_Theta * sin_phi
#       z <- radius * cos_Theta
#
#       azimuth <- phi * 180 / pi  # Azimuth angle in degrees
#       zenith <- (pi/2 - Theta) * 180 / pi  # Zenith angle in degrees
#
#       vertices[vertex_count, ] <- c(x, y, z, azimuth, zenith)
#       vertex_count <- vertex_count + 1
#     }
#   }
#
#   for (lat in 0:latitude_divisions) {
#     for (lon in 0:longitude_divisions) {
#       if (lat < latitude_divisions && lon < longitude_divisions) {
#         first <- lat * (longitude_divisions + 1) + lon + 1
#         second <- first + longitude_divisions + 1
#
#         indices[index_count, ] <- c(first, second, first + 1)
#         indices[index_count + 1, ] <- c(second, second + 1, first + 1)
#
#         index_count <- index_count + 2
#       }
#     }
#   }
#
#   return(
#     tibble(
#       x = vertices[,1],
#       y = vertices[,2],
#       z = vertices[,3],
#       Phi = vertices[,4],
#       Theta = vertices[,5]
#       ))
# }
#
# # Parameters for the sphere
# radius <- 1.0  # Radius of the sphere
# latitude_divisions <- 20  # Number of divisions along the latitude
# longitude_divisions <- 20  # Number of divisions along the longitude
#
# # Create the sphere mesh in Cartesian coordinates
# sphere_data <- create_cartesian_sphere_with_angles(radius, latitude_divisions, longitude_divisions)
#
# Function to plot circles in XY, XZ, and YZ planes
# create_3d_planes <- function(radius = 1) {
#   # Generate Theta values for the circles
#   Theta <- seq(0, 2 * pi, length.out = 100)
#
#   # Generate coordinates for circles in XY, XZ, and YZ planes
#   xy <- data.frame(
#     x = radius * cos(Theta),
#     y = radius * sin(Theta),
#     z = 0,
#     Theta = Theta,
#     Phi = rep(0, length(Theta)),
#     Plane = "xy",
#     Color = "blue"
#   )
#
#   xz <- data.frame(
#     x = radius * cos(Theta),
#     y = 0,
#     z = radius * sin(Theta),
#     Theta = Theta,
#     Phi = rep(0, length(Theta)),
#     Plane = "xz",
#     Color = "red"
#   )
#
#   yz <- data.frame(
#     x = 0,
#     y = radius * cos(Theta),
#     z = radius * sin(Theta),
#     Theta = rep(0, length(Theta)),
#     Phi = Theta,
#     Plane = "yz",
#     Color = "green"
#     )
#
#   merged <- bind_rows(xy, xz, yz) %>%
#     group_by(Plane)
#
#   return(merged)
# }

create_planes_data <- function(radius = 1, n_points = 100) {
  # Generate theta (azimuth) and phi (zenith) values for the points on the circles
  theta <- seq(0, 2 * pi, length.out = n_points)
  phi <- seq(-pi / 2, pi / 2, length.out = n_points)

  # Create XY circle data
  xy_circle <- expand.grid(theta = theta, phi = rep(0, n_points))
  xy_circle$x <- radius * cos(xy_circle$theta)
  xy_circle$y <- radius * sin(xy_circle$theta)
  xy_circle$z <- rep(0, n_points)
  xy_circle$Theta <- xy_circle$theta * 180 / pi
  xy_circle$Phi <- xy_circle$phi * 180 / pi
  xy_circle$Plane <- "XY"
  xy_circle$Color <- "blue"

  # Create XZ circle data
  xz_circle <- expand.grid(theta = c(rep(0 * pi / 180, n_points / 2), rep(180 * pi / 180, n_points / 2)), phi = phi)
  xz_circle$x <- radius * cos(xz_circle$phi) * cos(xz_circle$theta)
  xz_circle$y <- rep(0, n_points)
  xz_circle$z <- radius * sin(xz_circle$phi)
  xz_circle$Theta <- xz_circle$theta * 180 / pi
  xz_circle$Phi <- xz_circle$phi * 180 / pi
  xz_circle$Plane <- "XZ"
  xz_circle$Color <- "red"

  # Create YZ circle data
  yz_circle <- expand.grid(theta = c(rep(90 * pi / 180, n_points / 2), rep(270 * pi / 180, n_points / 2)), phi = phi)
  yz_circle$x <- rep(0, n_points)
  yz_circle$y <- radius * cos(yz_circle$phi) * sin(yz_circle$theta)
  yz_circle$z <- radius * sin(yz_circle$phi)
  yz_circle$Theta <- yz_circle$theta * 180 / pi
  yz_circle$Phi <- yz_circle$phi * 180 / pi
  yz_circle$Plane <- "YZ"
  yz_circle$Color <- "green"

  # Order the points by for each circle
  xy_circle <- xy_circle[order(xy_circle$theta), ]
  xz_circle <- xz_circle[order(xz_circle$theta, xz_circle$phi, xz_circle$x), ]
  yz_circle <- yz_circle[order(yz_circle$theta, yz_circle$phi, yz_circle$y), ]

  # Combine the data frames
  planes_data <- bind_rows(xy_circle, xz_circle, yz_circle)

  return(planes_data)
}

# Example usage
Plane3D <- create_planes_data(radius = 1, n_points = 50)

# Function to plot a sphere in polar coordinates with azimuth and zenith values
polar_sphere <- function(radius = 1, n = 45) {
  # Generate Theta (azimuth) and Phi (zenith) values for the points on the sphere
  Theta <- seq(0, 2 * pi, length.out = n)
  Phi <- seq(0, pi, length.out = n)

  # Generate coordinates for points on the sphere in polar coordinates
  PolarSphere <- expand.grid(Theta = Theta, Phi = Phi)
  PolarSphere$r <- rep(radius, n * n)

  # Convert polar coordinates to Cartesian coordinates
  PolarSphere$x <- PolarSphere$r * sin(PolarSphere$Phi) * cos(PolarSphere$Theta)
  PolarSphere$y <- PolarSphere$r * sin(PolarSphere$Phi) * sin(PolarSphere$Theta)
  PolarSphere$z <- PolarSphere$r * cos(PolarSphere$Phi)

  PolarSphere$Theta <- PolarSphere$Theta * 180 / pi
  PolarSphere$Phi <- (pi / 2 - PolarSphere$Phi) * 180 / pi

  return(PolarSphere)
}

PolarSphere <- polar_sphere(radius = 1, n = 46)

# Function to rotate the body frame using Tait-Bryan angles
rotate_body_frame <- function(heading, pitch, roll) {
  # Convert angles from degrees to radians
  heading_rad <- heading * pi / 180
  pitch_rad <- -pitch * pi / 180 # negate pitch nd roll to swhicth between Apllanix coordinate system and Plotly
  roll_rad <- -roll * pi / 180

  # Create the rotation matrices
  rotation_z <- matrix(c(cos(heading_rad), sin(heading_rad), 0, -sin(heading_rad), cos(heading_rad), 0, 0, 0, 1), nrow = 3)
  rotation_y <- matrix(c(cos(pitch_rad), 0, -sin(pitch_rad), 0, 1, 0, sin(pitch_rad), 0, cos(pitch_rad)), nrow = 3)
  rotation_x <- matrix(c(1, 0, 0, 0, cos(roll_rad), sin(roll_rad), 0, -sin(roll_rad), cos(roll_rad)), nrow = 3)

  # Rotate the body frame in the correct order: Z (Yaw) -> Y (pitch) -> X (roll)
  rotation_matrix <- rotation_z %*% rotation_y %*% rotation_x

  # Original body frame vectors (x, y, z)
  body_vectors <- matrix(c(1, 0, 0, 0, 1, 0, 0, 0, 1), nrow = 3) # Unit vectors

  # Rotate the body frame vectors
  rotated_vectors <- rotation_matrix %*% body_vectors

  rotated_vectors <- as_tibble(rotated_vectors) %>%
    mutate(
      Coord = c("x", "y", "z")
    ) %>%
    rename(X = V1, Y = V2, Z = V3) %>%
    pivot_wider(
      names_from = Coord,
      values_from = c(X, Y, Z),
      names_sep = ""
    )

  return(rotated_vectors)
}

test <- test %>%
  mutate(
    rotated_body = purrr::pmap(list(heading, pitch, roll), rotate_body_frame)
  ) %>%
  unnest(cols = c(rotated_body))

# Function to rotate a point using Tait-Bryan angles
rotate_point <- function(point, heading, pitch, roll) {
  # Convert angles from degrees to radians
  heading_rad <- heading * pi / 180
  pitch_rad <- -pitch * pi / 180 # negate pitch nd roll to swhicth between Apllanix coordinate system and Plotly
  roll_rad <- -roll * pi / 180

  # Create the rotation matrices
  rotation_z <- matrix(c(cos(heading_rad), sin(heading_rad), 0, -sin(heading_rad), cos(heading_rad), 0, 0, 0, 1), nrow = 3)
  rotation_y <- matrix(c(cos(pitch_rad), 0, -sin(pitch_rad), 0, 1, 0, sin(pitch_rad), 0, cos(pitch_rad)), nrow = 3)
  rotation_x <- matrix(c(1, 0, 0, 0, cos(roll_rad), sin(roll_rad), 0, -sin(roll_rad), cos(roll_rad)), nrow = 3)

  # Rotate the body frame in the correct order: Z (Yaw) -> Y (pitch) -> X (roll)
  rotated_point <- rotation_z %*% rotation_y %*% rotation_x %*% point

  rotated_point <- as_tibble(rotated_point) %>%
    mutate(
      Coord = c("x", "y", "z")
    ) %>%
    rename(EsPoint = V1) %>%
    pivot_wider(
      names_from = Coord,
      values_from = EsPoint,
      names_prefix = "EsPoint",
      names_sep = ""
    )


  return(rotated_point)
}

# Generate a point representing a point in the body frame
test <- test %>%
  mutate(
    EsPoint = list(matrix(c(0, 0, 0.5), nrow = 3)) # Along the Z-axis, could place it precisely about the IMU
  )

test <- test %>%
  mutate(
    RotatedPoint = purrr::pmap(list(EsPoint, heading, pitch, roll), rotate_point)
  ) %>%
  unnest(cols = c(RotatedPoint))
```

```{r}
fig <- test %>%
  plot_ly(type = "scatter3d", mode = "markers") %>%
  add_markers(
    name = "EsPosition",
    y = ~Zx, # reverse x and y to shift between applanix coordinate system and plotly
    x = ~Zy,
    z = ~Zz,
    symbol = "triangle-up-open",
    color = ~channel,
    colors = inferno(200),
    marker = list(
      size = 5
    ),
    text = ~ paste0(
      "<b>heading</b>: ", heading, "<br>",
      "<b>pitch</b>: ", pitch, "<br>",
      "<b>roll</b>: ", roll, "<br>",
      "<b>sol_zen</b>: ", sol_zen, "<br>",
      "<b>channel</b>: ", channel, "<br>"
    )
  ) %>%
  add_markers(
    name = "heading",
    y = ~Xx, # reverse x and y to shift between applanix coordinate system and plotly
    x = ~Xy,
    z = ~Xz,
    marker = list(
      color = "rgb(253, 0, 0)",
      size = 5,
      line = list(
        color = "rgb(0, 0, 0)",
        width = 1
      )
    ),
    text = ~ paste0(
      "<b>heading</b>: ", heading, "<br>",
      "<b>pitch</b>: ", pitch, "<br>",
      "<b>roll</b>: ", roll, "<br>",
      "<b>sol_zen</b>: ", sol_zen, "<br>",
      "<b>channel</b>: ", channel, "<br>"
    )
  ) %>%
  add_markers(
    name = "SolarPosition",
    y = ~Solx,
    x = ~Soly,
    z = ~Solz,
    # color = ~sol_zen,
    # colors = inferno(200),
    text = ~ paste0(
      "<b>sol_azi</b>: ", sol_azi, "<br>",
      "<b>sol_zen</b>: ", sol_zen, "<br>",
      "<b>date_time</b>: ", date_time, "<br>",
      "<b>boat_raa</b>: ", boat_raa, "<br>"
    ),
    marker = list(
      color = "rgb(253, 184, 19)",
      size = 10,
      line = list(
        color = "rgb(0, 0, 0)",
        width = 1
      )
    )
  ) %>%
  add_trace(
    y = ~ as.vector(rbind(0, Xx)),
    x = ~ as.vector(rbind(0, Xy)),
    z = ~ as.vector(rbind(0, Xz)),
    mode = "lines",
    line = list(color = "red"),
    name = "Rotated X"
  ) %>%
  add_trace(
    y = ~ as.vector(rbind(0, Yx)),
    x = ~ as.vector(rbind(0, Yy)),
    z = ~ as.vector(rbind(0, Yz)),
    mode = "lines",
    line = list(color = "green"),
    name = "Rotated Y"
  ) %>%
  add_trace(
    y = ~ as.vector(rbind(0, Zx)),
    x = ~ as.vector(rbind(0, Zy)),
    z = ~ as.vector(rbind(0, Zz)),
    mode = "lines",
    line = list(color = "purple"),
    name = "Rotated Z"
  ) %>%
  add_trace(y = c(0, 1), x = c(0, 0), z = c(0, 0), mode = "lines", line = list(color = "red"), name = "Original X") %>%
  add_trace(y = c(0, 0), x = c(0, 1), z = c(0, 0), mode = "lines", line = list(color = "green"), name = "Original Y") %>%
  add_trace(x = c(0, 0), y = c(0, 0), z = c(0, 1), mode = "lines", line = list(color = "purple"), name = "Original Z") %>%
  add_markers(
    data = PolarSphere %>% filter(z > 0),
    name = "MeshSphere",
    y = ~x,
    x = ~y,
    z = ~z,
    opacity = 0.5,
    marker = list(
      color = "rgb(201, 233, 246)",
      size = 2,
      line = list(
        color = "rgb(53, 81, 92)",
        width = 2
      )
    ),
    text = ~ paste0(
      "<b>Azimuth</b>: ", Theta, "<br>",
      "<b>Zenith</b>: ", Phi, "<br>"
    )
  ) %>%
  add_paths(
    data = Plane3D,
    y = ~x,
    x = ~y,
    z = ~z,
    color = ~Plane,
    colors = ~Color,
    text = ~ paste0(
      "<b>Azimuth</b>: ", Theta, "<br>",
      "<b>Zenith</b>: ", Phi, "<br>"
    )
  )
fig
```


#### Plot the Sun in the body frame

To make this work need to define clearly the body and the world frame.

```{r}
# Function to convert Tait-Bryan angles to a rotation matrix
tait_bryan_to_rotation_matrix <- function(roll, pitch, yaw) {
  roll_rad <- roll * pi / 180
  pitch_rad <- pitch * pi / 180
  yaw_rad <- yaw * pi / 180

  rotation_matrix <- matrix(0, nrow = 3, ncol = 3)

  rotation_matrix[1, 1] <- cos(yaw_rad) * cos(pitch_rad)
  rotation_matrix[1, 2] <- cos(yaw_rad) * sin(pitch_rad) * sin(roll_rad) - sin(yaw_rad) * cos(roll_rad)
  rotation_matrix[1, 3] <- cos(yaw_rad) * sin(pitch_rad) * cos(roll_rad) + sin(yaw_rad) * sin(roll_rad)
  rotation_matrix[2, 1] <- sin(yaw_rad) * cos(pitch_rad)
  rotation_matrix[2, 2] <- sin(yaw_rad) * sin(pitch_rad) * sin(roll_rad) + cos(yaw_rad) * cos(roll_rad)
  rotation_matrix[2, 3] <- sin(yaw_rad) * sin(pitch_rad) * cos(roll_rad) - cos(yaw_rad) * sin(roll_rad)
  rotation_matrix[3, 1] <- -sin(pitch_rad)
  rotation_matrix[3, 2] <- cos(pitch_rad) * sin(roll_rad)
  rotation_matrix[3, 3] <- cos(pitch_rad) * cos(roll_rad)

  return(rotation_matrix)
}

# Convert Tait-Bryan angles to rotation matrix

sol_direction_body <- function(RotationMatrixBody, Solx, Soly, Solz) {
  mat <- as_tibble(t(RotationMatrixBody %*% c(Solx, Soly, Solz)))
  colnames(mat) <- c("BodySolx", "BodySoly", "BodySolz")

  return(mat)
}

test <- test %>%
  mutate(
    RotationMatrixBody = pmap(list(roll, pitch, heading), tait_bryan_to_rotation_matrix),
    SolDirectionBody = pmap(list(RotationMatrixBody, Solx, Soly, Solz), sol_direction_body)
  ) %>%
  unnest(cols = c(SolDirectionBody))
```




## instruments selection and processing

instrument available to process are detected from the loaded files. For now the processing is tailored to the SeaDoo configuration.

*NEED* think about generalization of processing with different instruments configuration. 
usage of \*.con files (similar to ProSoft) ?

*OPTIONAL*
Could use ECMWF services (climate reanalysis) to add meteorological data to the interface.
Clouds overlay on the selection map ?
Clouds percentage, Wind speed in the area (?) as metadata ?

# Automatic processing

As the quantity of data acquired in continuous acquisition quickly become large, it is necessary to automatize the processing. Full automation is of course possible and in this case, the interactivity offered by *sear* would not matter much. However I think that the analyst need's to keep an eye (or at least be able to) on the data. Therefore the goals of this section are threefold:
* Developing the logic for automatic processing, from L1 to L2 including qc
* Preserving the analyst ability to judge things and to interact with the data
* Maintaining data consistency through and within the interaction of the human and the computer.

## time is of the essence

The algorithmic logic for the discretization based on time slicing is as follow:
1. Start from the first `date_time` (x) take the next `date_time` (y), with a minimum of three seconds difference.
2. If y is more than ten seconds away replace x by y+1 and start over at step 1
3. Try to process with that time `interval(x, y)`
4. If there is not enough data (HOCR), add +1 second to y and retry step 3.
5. If processing is successful, select the next x after y and continue from step one.

*TODO* check inclusive exclusive intervals ends.

```{r}
discretize_time <- function(Maintime) {
  i <- 1
  j <- 1
  while (T) {
    if (length(Maintime) < i + 1) {
      print("This is the end")
      break
    }

    j <- i

    x <- Maintime[i]
    y <- Maintime[j + 1]

    if (interval(x, y) > seconds(10)) {
      i <- j + 1
      next
    }

    # Step 1, iter until interval < 3 sec
    while (interval(x, y) < seconds(3)) {
      if (length(Maintime) < j + 1) {
        print("This is the end")
        break
      }

      j <- j + 1
      y <- Maintime[j]
    }

    # At this point. this should always be true
    # Step 3 and 4, processing should happen at this stage
    if (!interval(x, y) > seconds(10) & !interval(x, y) < seconds(3)) {
      print(paste(i, interval(x, y)))
    }

    i <- j + 1
  }
}

discretize_time(Maintime = MainLog$date_time)
```


# Server

## golem organization

This shiny app is becoming quite large. a clear organisation is the key to development and maintenance, golem is here to help in this task.

* fct_ : files containing the function of the app, they defined the data parsing, tidying and computation logic.
* mod_ : files containing both frontend (`_ui` function) and backend (`_server` function) shiny logic. Inputs, reactive value, observer and consumer are defined here.
* utils_ : files containing helpers functions that doesn't really belong exclusively to this app.
* app_ : files to wrap ui and server, configure and run the app.
* zzz.R : file containing the call to `.onLoad()`

## app_server

The app_server.R wrap up the application server side, it's where all modules are put together and communication between them is defined.
It's also there than global options such as `options(digits.secs = 3)` (to display and save time at the millisecond with 3 digit accuracy) are located.

# SQLite backend

Connectivity to the SQLite persistent data storage is defined in `mod_manage_db` and `mod_manage_obs`.

## DB schema

The SQLite scheme is defined in `mod_manage_db`.
Present the DB scheme here

## DB migration

As the app evolve and integrate new variables, old databases schema need to migrate to new schema. This section keep tracks of those changes and give the SQL migration scripts used.

Adapted from: https://stackoverflow.com/questions/4253804/how-do-i-add-a-new-column-in-between-two-columns

### V0.1.0 -> V0.2.0

Create new table with the added column, here Rrs_loess KLu_loess, RbI
```{sql}
CREATE TABLE NewHOCRL2 (
  `wavelength` REAL,
  `Rrs` REAL,
  `Rrs_loess` REAL,
  `KLu` REAL,
  `KLu_loess` REAL,
  `RbI` REAL,
  `uuid_l2` TEXT,
  FOREIGN KEY (uuid_l2)
    REFERENCES Metadata (uuid_l2)
    ON DELETE CASCADE
);
```

Insert the old data in the new table
```{sql}
INSERT INTO NewHOCRL2 (
  `wavelength`,
  `Rrs`,
  `Rrs_loess`,
  `KLu`,
  `KLu_loess`,
  `uuid_l2`
) SELECT   
  `wavelength`,
  `Rrs`,
  `Rrs_loess`,
  `KLu`,
  `KLu_loess`,
  `uuid_l2`
FROM hocr_l2;
```

*Check that the insertion is correct*
Delete the old table.
```{sql}
DROP TABLE hocr_l2;
```

Rename the new table.
```{sql}
ALTER TABLE NewHOCRL2 RENAME TO hocr_l2;
```

# SQLite frontend

## Save

When an `uuid_l2` already exist and the `Save` button is pressed, the records in the SQLite DB are uptaded. For the `Metadata` table, the following fields are updated:
* `comment`
* `date_time_processing`
* `analyst`
* `mail`

For the `hocr_l1b` table only the `qc` field is updated, it is uniquely identified by the (uuid_l2, id) pair. THIS PAIR IS NOT a PRIMARY KEY as it doesn't uniquely identifies the entry of the table. The PRIMARY KEY of the hocr_l1b table is (uuid_l2, id, sn, wavelength).
QUESTION: Would it be relevant to separate the `qc` in it's own table ?

When the `Save` button is pressed, feedback is given to the user in the form of a `session$sendCustomMessage`.

Note reagarding diffference bettwen the `WHERE` and the `CASE WHEN` approach for updating SQLite.
Whith

## Delete

When the delete button is pressed, if the active data point (station) has an `uuid_l2`, it's suppressed in the `Metadata` table. As the FOREIGN KEY constraint is activated in the DB schema (`PRAGMA foreign_keys=ON`) and `hocr_l1b` and `hocr_l2` are child table build with the `ON DELETE CASCADE` constraints, matching record are automatically deleted.
