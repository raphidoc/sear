---
title: "sear_dev"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{sear_dev}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette intend to document the development and the functionality of the
`sear_app()` function. This function is the top level function that call the
shiny app designed to process the data from the watercraft HOCR, BB3, SeaOWL, CTD and SUNA.

```{r setup}
golem::document_and_reload()

library(sear)
library(dplyr)
library(lubridate)
library(purrr)
library(plotly)
library(readr)
library(stringr)
library(tidyr)
library(suncalc)
```

```{r}
ConFile <- system.file("extdata", "DATA_20220705_105732.txt", package = "sear", mustWork = T)

MTELog <- sear:::read_mtelog(ConFile)

Apla <- sear:::read_apla(MTELog)

SBE19 <- sear:::read_sbe19(MTELog)

BinFile <- system.file("extdata", "DATA_20220705_105732.bin", package = "sear", mustWork = T)

HOCR <- sear:::read_hocr(BinFile)

AplaDate <- unique(date(Apla$DateTime))

HOCRTimeIndex <- purrr::map(
  .x = HOCR,
  ~ clock::date_time_parse(
    paste0(AplaDate, " ", hms::as_hms(.x$gpstime / 1000)),
    zone = "UTC")
)
```
# Foreword

The idea behind this application software originated from a conversation with myself during the first year of my master thesis.

> Do we have a way to manage and process data in a usable and consistent way ? No. Ok so it's On !

* [https://en.wikipedia.org/wiki/Usability](Usability) :

> Capacity of a system to provide a condition for its users to perform the tasks safely, effectively, and efficiently while enjoying the experience.

* [https://en.wikipedia.org/wiki/Consistency_(disambiguation)](Consistency) :

> A consistent theory is one that does not lead to a logical contradiction.

Software as a Service offer a solution to have a common architecture and code base to be shared among people while suiting their specific need. When we think about managing and processing different data type of various format coming from different instruments, this mix between the rigidity of a consistent architecture and the flexibility required to solve various problems is best solved today by web-based application. That's why sear is such an application.

This software is originally developed and tailored for the Sea-Doo/MTE data logger, but it could provide a common ground in ocean optics science data management and processing. If your willing for us (Ocean Optic Researcher) to meet halfway somewhere, it could be here. Future issues, necessary decision and trade of could bring this community closer and help us move toward a more collaborative future (e.g. NASA TOPS program). We may be able to leave behind the need to reinvent the wheel at each new generation of young scientist.
(Also it's often by reinventing the wheel that you understand how it work and that you can appropriate some principle to develop new idea)

# Dev Test and Demo dataset

Because of continuous acquisition, the HOCR SeaDoo data recorded by the MTE data logger is quite large (46.6M) and take time to parse from binary (~5.20 min giving 1 min to load 8.9M with a python virtualenv).
To parse and load the full AlgaeWise dataset (195.8M) it take 22 min just to parse HOCR, the other time consuming operation is to create the data synthesis for each instrument (see section [MainLog]), for now MainLog is recreated from scratch each time data is added when loading the final .bin and .txt files, it takes XXX minutes to compute.
To develop, test and demonstrate the software capabilities, a small data set is needed. As the most complete to date (2022), with the full data logger suit (except SUNA) plus BioSonic and r2Sonic, and spanning over multiple days, the AlgaeWise(2022) data offer the best option.

## Reduce raw data

To reduce the raw .bin and .txt data we start by identifying with the help of the app, on the full parsed data set, the time interval of the observation that we would like to keep.
We also identifies the position of the corresponding HOCR binary packets on the time index.

```{r}
full <- "/D/Data/sear_test_data/full/"
reduced <- "/D/Data/sear_test_data/reduced/"
```


TODO: Describe the selected data and explain it's utility (the logic of the selection)

* Each different day to showcase the filtering and selection capabilities
* Shallow (~1.5m) and deep water (~20m)
* Full range of boat solar azimuth

```{r eval=FALSE, include=TRUE}
# Cloudy and wave that day
D02 <- list(
  strftime(seq(ymd_hms("2022-07-02 16:31:00"), ymd_hms("2022-07-02 16:32:00"), by = 1), format="%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-02 19:08:23"), ymd_hms("2022-07-02 19:08:45"), by = 1), format="%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-02 19:32:51"), ymd_hms("2022-07-02 19:33:26"), by = 1), format="%H:%M:%S", tz = "GMT")
  )

D05 <- list(
  strftime(seq(ymd_hms("2022-07-05 11:50:27"), ymd_hms("2022-07-05 11:51:02"), by = 1), format="%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-05 14:14:51"), ymd_hms("2022-07-05 14:21:16"), by = 1), format="%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-05 14:40:20"), ymd_hms("2022-07-05 14:40:47"), by = 1), format="%H:%M:%S", tz = "GMT")
  )

D0615 <- list(
  strftime(seq(ymd_hms("2022-07-06 15:48:00"), ymd_hms("2022-07-06 15:55:00"), by = 1), format="%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-06 16:54:45"), ymd_hms("2022-07-06 17:02:41"), by = 1), format="%H:%M:%S", tz = "GMT")
  )

D0618 <- list(
  strftime(seq(ymd_hms("2022-07-06 20:26:50"), ymd_hms("2022-07-06 20:29:28"), by = 1), format="%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-06 20:48:26"), ymd_hms("2022-07-06 20:50:28"), by = 1), format="%H:%M:%S", tz = "GMT")
  )

D07 <- list(
  strftime(seq(ymd_hms("2022-07-07 19:26:04"), ymd_hms("2022-07-07 19:38:31"), by = 1), format="%H:%M:%S", tz = "GMT")

  )

D08 <- list(
  strftime(seq(ymd_hms("2022-07-08 19:10:42"), ymd_hms("2022-07-08 19:14:52"), by = 1), format="%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-08 19:36:28"), ymd_hms("2022-07-08 19:39:19"), by = 1), format="%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-08 20:47:42"), ymd_hms("2022-07-08 20:50:10"), by = 1), format="%H:%M:%S", tz = "GMT"),
  strftime(seq(ymd_hms("2022-07-08 20:53:44"), ymd_hms("2022-07-08 20:56:34"), by = 1), format="%H:%M:%S", tz = "GMT")
  )

```

### Reducing txt files. Easy Peasy Lemon Squeezy !

```{r eval=FALSE, include=TRUE}
trim_MTELog <- function(If, Of, TimeSeq, n_head){

  Head <- readr::read_lines(If, n_max = n_head)
  readr::write_lines(Head, Of)

  Data <- readr::read_lines(If, skip = n_head)

  Pat <- str_c(unlist(TimeSeq), collapse = "|")
  Tind <- str_detect(Data, Pat)
  any(Tind)
  readr::write_lines(Data[Tind], Of, append = T)
}

If <- file.path(full,"DATA_20220702_154124.txt")
Of <- file.path(reduced,"DATA_20220702_154124.txt")
trim_MTELog(If,Of, D02, n_head = 6)

If <- file.path(full,"DATA_20220705_105732.txt")
Of <- file.path(reduced,"DATA_20220705_105732.txt")
trim_MTELog(If,Of,D05, n_head = 6)  

If <- file.path(full,"DATA_20220706_153838.txt")
Of <- file.path(reduced,"DATA_20220706_153838.txt")
trim_MTELog(If,Of,D0615, n_head = 6)

If <- file.path(full,"DATA_20220706_183905.txt")
Of <- file.path(reduced,"DATA_20220706_183905.txt")
trim_MTELog(If,Of,D0618, n_head = 6)  


If <- file.path(full,"DATA_20220707_181725.txt")
Of <- file.path(reduced,"DATA_20220707_181725.txt")
trim_MTELog(If,Of,D07, n_head = 6)

If <- file.path(full,"DATA_20220708_165254.txt")
Of <- file.path(reduced,"DATA_20220708_165254.txt")
trim_MTELog(If,Of,D08, n_head = 6)


If <- file.path(full,"SBES_20220701_20220708.csv")
Of <- file.path(reduced,"SBES_20220701_20220708.csv")
trim_MTELog(If,Of,list(D02,D05,D0615,D0618,D07,D08), n_head = 1)
```

### Reducing HOCR binary file ... Raaaaagh ...

Count Number of byte by file, divide by number of byte per packets, find position of packets to extract with the time index.

I just realize now that the header size is equal to any packet size (512 bytes). Now it make a lot of sense why there was so much empty space after the header !
The fixed size memory entry is 512 bytes. I would like to learn more about that !
Wah ! The `dd` command use 512 as a default number of bytes for the `-bs` argument !
Whats behind this number !?

```{r eval=FALSE, include=TRUE}
trim_hocr <- function(If, Of, TimeIndex, TimeInt) {

  # Copy header
  system2('dd', paste0("if=",If, " bs=512", " count=",1, " >>",Of))

  for (Tint in TimeInt) {
    print(Tint)

    ind <- which(purrr::map_lgl(.x = TimeIndex, ~ .x %within% Tint))
    BytesStart <- min(ind)#*512
    BytesEnd <- max(ind)#*512
    Skip <- BytesStart-1
    Count <- BytesEnd-BytesStart

    print(Skip)
    print(Count)

    # Append matching packets
    system2('dd', paste0("if=",If, " bs=512", " skip=",Skip, " count=",Count, " >>",Of))

  }
}

#MTEHeader <- 512
MTEBytes <- 512

######### F02

# use  < to remove the filename
F02bin <- file.path(full,"DATA_20220702_154124.bin")
F02out <- file.path(reduced,"DATA_20220702_154124.bin")
F02Nbytes <- as.numeric(system2('wc', paste0("-c < ", F02bin), stdout = T))

F02Nbytes/MTEBytes
# 84409

RawHOCR <- sear:::read_hocr(F02bin)
#Invalid HOCR packets detected and removed: 17
# 84409 - 17 - 1 = 84391 yeah !

TimeIndex <- purrr::map(
  .x = RawHOCR,
  ~ clock::date_time_parse(
    paste0(ymd("20220702"), " ", hms::as_hms(.x$gpstime / 1000)),
    zone = "UTC")
)

TimeInt <- list(
  interval(ymd_hms("2022-07-02 16:31:00"), ymd_hms("2022-07-02 16:32:00")),
  interval(ymd_hms("2022-07-02 19:08:23"), ymd_hms("2022-07-02 19:08:45")),
  interval(ymd_hms("2022-07-02 19:32:51"), ymd_hms("2022-07-02 19:33:26"))
  )

trim_hocr(If = F02bin, Of = F02out, TimeIndex, TimeInt)

######### F05

# use  < to remove the filename
F05bin <- file.path(full,"DATA_20220705_105732.bin")
F05out <- file.path(reduced,"DATA_20220705_105732.bin")

RawHOCR <- sear:::read_hocr(F05bin)
#Invalid HOCR packets detected and removed: 17
# 84409 - 17 - 1 = 84391 yeah !

TimeIndex <- purrr::map(
  .x = RawHOCR,
  ~ clock::date_time_parse(
    paste0(ymd("20220705"), " ", hms::as_hms(.x$gpstime / 1000)),
    zone = "UTC")
)

TimeInt <- list(
  interval(ymd_hms("2022-07-05 11:50:27"), ymd_hms("2022-07-05 11:51:02")),
  interval(ymd_hms("2022-07-05 14:14:51"), ymd_hms("2022-07-05 14:21:16")),
  interval(ymd_hms("2022-07-05 14:40:20"), ymd_hms("2022-07-05 14:40:47"))
  )

trim_hocr(If = F05bin, Of = F05out, TimeIndex, TimeInt)

######### F0615

# use  < to remove the filename
F0615bin <- file.path(full,"DATA_20220706_153838.bin")
F0615out <- file.path(reduced,"DATA_20220706_153838.bin")

RawHOCR <- sear:::read_hocr(F0615bin)
#Invalid HOCR packets detected and removed: 17
# 84409 - 17 - 1 = 84391 yeah !

TimeIndex <- purrr::map(
  .x = RawHOCR,
  ~ clock::date_time_parse(
    paste0(ymd("20220706"), " ", hms::as_hms(.x$gpstime / 1000)),
    zone = "UTC")
)

TimeInt <- list(
  interval(ymd_hms("2022-07-06 15:48:00"), ymd_hms("2022-07-06 15:55:00")),
  interval(ymd_hms("2022-07-06 16:54:45"), ymd_hms("2022-07-06 17:02:41"))
  )

trim_hocr(If = F0615bin, Of = F0615out, TimeIndex, TimeInt)

######### F0618

# use  < to remove the filename
F0618bin <- file.path(full,"DATA_20220706_183905.bin")
F0618out <- file.path(reduced,"DATA_20220706_183905.bin")

RawHOCR <- sear:::read_hocr(F0618bin)
#Invalid HOCR packets detected and removed: 17
# 84409 - 17 - 1 = 84391 yeah !

TimeIndex <- purrr::map(
  .x = RawHOCR,
  ~ clock::date_time_parse(
    paste0(ymd("20220706"), " ", hms::as_hms(.x$gpstime / 1000)),
    zone = "UTC")
)

TimeInt <- list(
  interval(ymd_hms("2022-07-06 20:26:50"), ymd_hms("2022-07-06 20:29:28")),
  interval(ymd_hms("2022-07-06 20:48:26"), ymd_hms("2022-07-06 20:50:28"))
  )

trim_hocr(If = F0618bin, Of = F0618out, TimeIndex, TimeInt)

######### F07

# use  < to remove the filename
F07bin <- file.path(full,"DATA_20220707_181725.bin")
F07out <- file.path(reduced,"DATA_20220707_181725.bin")

RawHOCR <- sear:::read_hocr(F07bin)
#Invalid HOCR packets detected and removed: 17
# 84409 - 17 - 1 = 84391 yeah !

TimeIndex <- purrr::map(
  .x = RawHOCR,
  ~ clock::date_time_parse(
    paste0(ymd("20220707"), " ", hms::as_hms(.x$gpstime / 1000)),
    zone = "UTC")
)

TimeInt <- list(
  interval(ymd_hms("2022-07-07 19:26:04"), ymd_hms("2022-07-07 19:38:31"))
  )

trim_hocr(If = F07bin, Of = F07out, TimeIndex, TimeInt)

######### F08

# use  < to remove the filename
F08bin <- file.path(full,"DATA_20220708_165254.bin")
F08out <- file.path(reduced,"DATA_20220708_165254.bin")

RawHOCR <- sear:::read_hocr(F08bin)
#Invalid HOCR packets detected and removed: 17
# 84409 - 17 - 1 = 84391 yeah !

TimeIndex <- purrr::map(
  .x = RawHOCR,
  ~ clock::date_time_parse(
    paste0(ymd("20220708"), " ", hms::as_hms(.x$gpstime / 1000)),
    zone = "UTC")
)

TimeInt <- list(
  interval(ymd_hms("2022-07-08 19:10:42"), ymd_hms("2022-07-08 19:14:52")),
  interval(ymd_hms("2022-07-08 19:36:28"), ymd_hms("2022-07-08 19:39:19")),
  interval(ymd_hms("2022-07-08 20:47:42"), ymd_hms("2022-07-08 20:50:10")),
  interval(ymd_hms("2022-07-08 20:53:44"), ymd_hms("2022-07-08 20:56:34"))
  )

trim_hocr(If = F08bin, Of = F08out, TimeIndex, TimeInt)
```

# UI

## Style

The UI use the 'norwegian blue' (The dead parrot) #4f89b0, rgb(79, 137, 176) as theme color

## Project workflow

The first step to work with `sear` is to select a project folder. A project folder is identified by the presence of a `.searproj` file, this file additionally contain relevant metadata such as the project path, date of creation and last update. Other useful information could be added.
If the selected folder does not contains `.searproj` file, one is created. The root of the project is set as the folder in which this file live.

The app will also look for folder and data files to re-initiate the app state as it was in the last processing save.

## Load Data

### MTE data logger

If no data where previously loaded, you need to provide some to the app. For now, only data from MTE datalogger can be parsed to populate the app. The raw MTE files (.bin and .txt) are saved in `.sear/data/raw`. On load, the app will parse and save the resulting applanix data, HOCR packet, HOCR dark packet, HOCR Time index, SBE19, SeaOwl and BBFL2 in `.sear/data/parsed`. This allow to save considerable time when re-opening the app as the bulk files are quite large and parsing them take time (~2/3 min for .bin = 46 MB).
If parsed data is already present, it will be read at the project selection.

#### HOCR specific files

The HOCR time index optimize processing time for the selection of HOCR packet within a time interval.
The HOCR dark is a list of shutter dark values interpolated to the minute to apply dark correction.

### BioSonic

Just select the .csv file processed with the dedicated software (biosonic Visual Aquatic).

TODO: Check the reference for the elevation values outputted by this software

### Hydroball

*COULD BE DONE*

## Selection display

### MainLog

When parsed data is loaded in the app, the `MainLog` is populated from the positioning system of the instruments package. For now only the SeaDoo Applanix can populate the `MainLog`. During the `MainLog` population (event dependent on `DataParsed`), a data synthesis is calculated from the `DateTime` values of each instrument. As this per-row operation takes time, `MainLog` is saved under `.sear/data/parsed/main_log_[:digit:]{8}_[:digit:]{6}.csv`.

TODO: could be improved by computing only for the new piece of Apla data that is added (but still on the complete time index of instrument) or could filter the time index of instrument by the interval of min and max time of the new piece of apla !

```{r}
MainLog <- Apla %>%
  rename(Lon = Lon_DD, Lat = Lat_DD)


data_synthesis_any_near <- function(x, y, tol = 3) {
  purrr::map_lgl(
          .x = x,
          ~ any(near(as.numeric(.x), as.numeric(y), tol = tol))
          )
}

SyntSBE19 <- data_synthesis_any_near(MainLog$DateTime, SBE19$DateTime)

SyntHOCR <- data_synthesis_any_near(MainLog$DateTime, HOCRTimeIndex)

microbenchmark(times = 1,
  data_synthesis_any_near(MainLog$DateTime, HOCRTimeIndex)
)
```

From this `MainLog`, the selection display column in the dashboard body is populated. This selection display is composed of a map which expose the discrete (aggregated to second) data point, of several filters (speed, time, need to add pitch and roll when available) and of a polar plot representing the solar azimuth relative to the boat heading of points selected on the map.

The points selected on the plotly map (lasso or box) will be processed when the `Process to L1b` button is clicked.

### NMEA string

TODO: List the required and optional NMEA string that sear will use.

*Mandatory*:
*GPGGA
*GPVTG

### DateTime filtering

DateTime classe is huge to store as is. As number of second since origin it is up to 100 times smaller.

*MANDATORY*

### Speed

*MANDATORY*: can be directly taken from the appropriate (GPVTG) NMEA string, or could be computed from GPGGA ?

### Azimuth polar plot

*MANDATORY*: boat azimuth can be taken from the appropriate (GPVTG) NMEA string, or could be computed from GPGGA ?

First compute the sun azimuth.

```{r}
# Apla <- Apla %>% rename(date = DateTime, lat = Lat_DD, lon = Lon_DD)
#
# # Solar altitude above the horizon in radian and azimuth in radian from south to west
# PosSol <- suncalc::getSunlightPosition(data = Apla, keep = c("altitude", "azimuth"))
#
# Apla <- left_join(Apla, PosSol, by = c("date", "lat", "lon")) %>%
#   rename(DateTime = date, Lat_DD = lat, Lon_DD = lon, SolAzm = azimuth, SolAlt = altitude) %>%
#   mutate(SolAzm = SolAzm * 180/pi + 180, # convert rad to degree and shift to north reference
#          SolAlt = SolAlt * 180/pi)
```

Take the boat azimuth (relative to true north) from the applanix, then subtract one by the other to find boat azimuth from the sun.

```{r}
Station1PaP <- interval(ymd_hms("2021-11-11 17:50:31"), ymd_hms("2021-11-11 17:50:40"))

temp <- Apla %>% filter(DateTime %within% Station1PaP)

# Apla <- Apla %>% mutate(
#   BoatSolAzm = SolAzm-Course_TN
# )

m <- list(
  l = 40,
  r = 30,
  b = 30,
  t = 30,
  pad = 0
)

temp %>%
  plot_ly(
    type = "scatterpolar",
    r = ~Speed_N,
    theta = ~BoatSolAzm,
    mode = "markers"
  ) %>%
  layout(
    autosize = F,
    margin = m,
    polar = list(
      angularaxis = list(
        # rotation = ~mean(Course_TN, na.rm = T)+90,
        direction = "clockwise"
      )
    )
  )
```

### Pitch and roll

*OPTIONAL*

## Instruments selection and processing

Instrument available to process are detected from the loaded files. For now the processing is tailored to the SeaDoo configuration.

*NEED* think about generalization of processing with different instruments configuration, usage of \*.con files (similar to ProSoft)

*OPTIONAL*
Could use ECMWF services (climate reanalysis) to add meteorological data to the interface. Like clouds on the selection map ?

# Server

## golem organization

This shiny app is becoming quite large. A clear organisation is the key to development and maintenance, golem is here to help in this task.

* fct_ : files containing the function of the app, they defined the data parsing, tidying and computation logic.
* mod_ : files containing both frontend (`_ui` function) and backend (`_server` function) shiny logic. Inputs, reactive value, observer and consumer are defined here.
* utils_ : files containing helpers functions that doesn't really belong exclusively to this app.
* app_ : files to wrap ui and server, configure and run the app.
* zzz.R : file containing the call to `.onLoad()`

## app_server

The app_server.R wrap up the application server side, it's where all modules are put together and communication between them is defined.
It's also there than global options such as `options(digits.secs = 3)` (to display and save time at the millisecond with 3 digit accuracy) are located.

# SQLite backend

Connectivity to the SQLite persistent data storage is defined in `mod_manage_db` and `mod_manage_obs`.

## DB schema

The SQLite scheme is defined in `mod_manage_db`.
Present the DB scheme here

## DB migration

As the app evolve and integrate new variables, old databases schema need to migrate to new schema. This section keep tracks of those changes and give the SQL migration scripts used.

Adapted from: https://stackoverflow.com/questions/4253804/how-do-i-add-a-new-column-in-between-two-columns

### V0.1.0 -> 

Create new table with the added column, here KLu_loess
```{sql}
CREATE TABLE NewHOCRL2 (
  `Wavelength` REAL,
  `Rrs` REAL,
  `KLu` REAL,
  `KLu_loess` REAL,
  `UUID` TEXT,
  FOREIGN KEY (UUID)
    REFERENCES Metadata (UUID)
    ON DELETE CASCADE
);
```

Insert the old data in the new table
```{sql}
INSERT INTO NewHOCRL2 (
  `Wavelength` REAL,
  `Rrs` REAL,
  `KLu` REAL,
  `UUID` TEXT
) SELECT   
  `Wavelength` REAL,
  `Rrs` REAL,
  `KLu` REAL,
  `UUID` TEXT
FROM HOCRL2;
```

*Check that the insertion is correct*
Delete the old table.
```{sql}
DROP TABLE HOCRL2;
```

Rename the new table.
```{sql}
ALTER TABLE NewHOCRL2 RENAME TO HOCRL2;
```


# SQLite frontend

## Save

When an `UUID` already exist and the `Save` button is pressed, the records in the SQLite DB are uptaded. For the `Metadata` table, the following fields are updated:
* `Comment`
* `ProTime`
* `Analyst`
* `Mail`

For the `HOCRL1b` table only the `QC` field is updated, it is uniquely identified by the (UUID, ID) pair. THIS PAIR IS NOT A PRIMARY KEY as it doesn't uniquely identifies the entry of the table. The PRIMARY KEY of the HOCRL1b table is (UUID, ID, SN, Wavelength).
QUESTION: Would it be relevant to separate the `QC` in it's own table ?

When the `Save` button is pressed, feedback is given to the user in the form of a `session$sendCustomMessage`.

Note reagarding diffference bettwen the `WHERE` and the `CASE WHEN` approach for updating SQLite.
Whith

## Delete

When the delete button is pressed, if the active data point (station) has an `UUID`, it's suppressed in the `Metadata` table. As the FOREIGN KEY constraint is activated in the DB schema (`PRAGMA foreign_keys=ON`) and `HOCRL1b` and `HOCRL2` are child table build with the `ON DELETE CASCADE` constraints, matching record are automatically deleted.
