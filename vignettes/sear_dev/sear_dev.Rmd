---
title: "sear_dev"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{sear_dev}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette intend to document the development and the functionality of the 
`sear_app()` function. This function is the top level function that call the
shiny app designed to process the data from the watercraft HOCR, BB3, SeaOWL, CTD and SUNA.

```{r setup}
golem::document_and_reload()

library(sear)
library(dplyr)
library(lubridate)
library(purrr)
library(plotly)
library(readr)
library(stringr)
library(tidyr)
library(suncalc)
```

```{r}
ConFile <- system.file("extdata", "DATA_20220705_105732.txt", package = "sear", mustWork = T)

MTELog <- sear:::read_mtelog(ConFile)

Apla <- sear:::read_apla(MTELog)

SBE19 <- sear:::read_sbe19(MTELog)

BinFile <- system.file("extdata", "DATA_20220705_105732.bin", package = "sear", mustWork = T)

HOCR <- sear:::read_hocr(BinFile)

AplaDate <- unique(date(Apla$DateTime))

HOCRTimeIndex <- purrr::map(
  .x = HOCR,
  ~ clock::date_time_parse(
    paste0(AplaDate, " ", hms::as_hms(.x$gpstime / 1000)),
    zone = "UTC")
)
```
# Foreword

The idea behind this application software originated from a conversation with myself during the first year of my master thesis. 

> Do we have a way to manage and process data in a usable and consistent way ? No. Ok so it's On !

* [https://en.wikipedia.org/wiki/Usability](Usability) : 

> Capacity of a system to provide a condition for its users to perform the tasks safely, effectively, and efficiently while enjoying the experience.

* [https://en.wikipedia.org/wiki/Consistency_(disambiguation)](Consistency) : 

> A consistent theory is one that does not lead to a logical contradiction.

# Dev, test and demo

A small dataset is needed for developing, testing and demonstrating the app.
This dataset must be representative of real wolrd data with flaws (missing byte, ...)
It must allow to exploit the tools of the app at their fullest (filtering and processing combinations).

For this purpose we will used a reduced version of the AlgaeWise data.

This section keeps track of the code used to produce this dataset.

## Raw

The first step is to identified on a full version of this dataset the data that will be included in the reduced version.
For that we use time interval by raw file.

The resulting trimmed raw data is stored in `extdata/demo/raw`. 

```{r}
D02 <- interval(ymd_hms("2022-07-05 12:23:13"), ymd_hms("2022-07-05 12:23:15"))
D05 <- interval(ymd_hms("2022-07-05 12:23:13"), ymd_hms("2022-07-05 12:23:15")) 
D0615 <- interval(ymd_hms("2022-07-05 12:23:13"), ymd_hms("2022-07-05 12:23:15")) 
D0618 <- interval(ymd_hms("2022-07-05 12:23:13"), ymd_hms("2022-07-05 12:23:15")) 
D07 <- interval(ymd_hms("2022-07-05 12:23:13"), ymd_hms("2022-07-05 12:23:15")) 
D08 <- interval(ymd_hms("2022-07-05 12:23:13"), ymd_hms("2022-07-05 12:23:15")) 
```

### SeaDooMTE

To filter the .txt files we apply a function on each one with the corresponding interval

To filter the .bin files we guess the position of the relevant packets by packets length, approximate number of packets per second, total number of packet and recording start time.

### BioSonic

To filter the .csv file we filter for data not falling in any interval with `case_when` (vectorized `if else`)

## Parsed

This reduced raw data is then parse with the `sear` function and saved in `extdata/demo/parsed`

## SQLite

TODO

# UI

## Style

The UI use the 'norwegian blue' (The dead parrot) #4f89b0, rgb(79, 137, 176) as theme color

## Project workflow

The first step to work with `sear` is to select a project folder. A project folder is identified by the presence of a `.searproj` file, this file additionally contain relevant metadata such as the project path, date of creation and last update. Other useful information could be added.
If the selected folder does not contains `.searproj` file, one is created. The root of the project is set as the folder in which this file live.

The app will also look for folder and data files to re-initiate the app state as it was in the last processing save. 

## Load Data

### MTE data logger

If no data where previously loaded, you need to provide some to the app. For now, only data from MTE datalogger can be parsed to populate the app. The raw MTE files (.bin and .txt) are saved in `.sear/data/raw`. On load, the app will parse and save the resulting applanix data, HOCR packet, HOCR dark packet, HOCR Time index, SBE19, SeaOwl and BBFL2 in `.sear/data/parsed`. This allow to save considerable time when re-opening the app as the bulk files are quite large and parsing them take time (~2/3 min for .bin = 46 MB).
If parsed data is already present, it will be read at the project selection. 

#### HOCR specific files

The HOCR time index optimize processing time for the selection of HOCR packet within a time interval.
The HOCR dark is a list of shutter dark values interpolated to the minute to apply dark correction.

### BioSonic

Just select the .csv file processed with the dedicated software (biosonic Visual Aquatic)

### Hydroball

*COULD BE DONE*

## Selection display

When parsed data is loaded in the app, the `MainLog` is populated from the positioning system of the instruments package. For now only the SeaDoo Applanix can populate the `MainLog`. During the `MainLog` population (event dependent on `DataParsed`), a data synthesis is calculated from the `DateTime` values of each instrument. As this per-row operation takes time, `MainLog` is saved under `.sear/data/parsed/main_log_[:digit:]{8}_[:digit:]{6}.csv`. 

```{r}
MainLog <- Apla %>%
  rename(Lon = Lon_DD, Lat = Lat_DD)


data_synthesis_any_near <- function(x, y, tol = 3) {
  purrr::map_lgl(
          .x = x,
          ~ any(near(as.numeric(.x), as.numeric(y), tol = tol))
          )
}

SyntSBE19 <- data_synthesis_any_near(MainLog$DateTime, SBE19$DateTime)

SyntHOCR <- data_synthesis_any_near(MainLog$DateTime, HOCRTimeIndex)

microbenchmark(times = 1,
  data_synthesis_any_near(MainLog$DateTime, HOCRTimeIndex)
)
```

From this `MainLog`, the selection display column in the dashboard body is populated. This selection display is composed of a map which expose the discrete (aggregated to second) data point, of several filters (speed, time, need to add pitch and roll when available) and of a polar plot representing the solar azimuth relative to the boat heading of points selected on the map.

The points selected on the plotly map (lasso or box) will be processed when the `Process to L1b` button is clicked.

NEED: List the required and optional NMEA string that sear will use.

-GPGGA

### DateTime filtering

DateTime classe is huge to store as is. As number of second since origin it is up to 100 times smaller.

*MANDATORY*

### Speed

*MANDATORY*: can be directly taken from the appropriate (GPVTG) NMEA string, or could be computed from GPGGA ?

### Azimuth polar plot

*MANDATORY*: boat azimuth can be taken from the appropriate (GPVTG) NMEA string, or could be computed from GPGGA ?

First compute the sun azimuth.

```{r}
# Apla <- Apla %>% rename(date = DateTime, lat = Lat_DD, lon = Lon_DD)
#
# # Solar altitude above the horizon in radian and azimuth in radian from south to west
# PosSol <- suncalc::getSunlightPosition(data = Apla, keep = c("altitude", "azimuth"))
#
# Apla <- left_join(Apla, PosSol, by = c("date", "lat", "lon")) %>%
#   rename(DateTime = date, Lat_DD = lat, Lon_DD = lon, SolAzm = azimuth, SolAlt = altitude) %>%
#   mutate(SolAzm = SolAzm * 180/pi + 180, # convert rad to degree and shift to north reference
#          SolAlt = SolAlt * 180/pi)
```

Take the boat azimuth (relative to true north) from the applanix, then subtract one by the other to find boat azimuth from the sun.

```{r}
Station1PaP <- interval(ymd_hms("2021-11-11 17:50:31"), ymd_hms("2021-11-11 17:50:40"))

temp <- Apla %>% filter(DateTime %within% Station1PaP)

# Apla <- Apla %>% mutate(
#   BoatSolAzm = SolAzm-Course_TN
# )

m <- list(
  l = 40,
  r = 30,
  b = 30,
  t = 30,
  pad = 0
)

temp %>%
  plot_ly(
    type = "scatterpolar",
    r = ~Speed_N,
    theta = ~BoatSolAzm,
    mode = "markers"
  ) %>%
  layout(
    autosize = F,
    margin = m,
    polar = list(
      angularaxis = list(
        # rotation = ~mean(Course_TN, na.rm = T)+90,
        direction = "clockwise"
      )
    )
  )
```

### Pitch and roll

*OPTIONAL*

## Instruments selection and processing

Instrument available to process are detected from the loaded files. For now the processing is tailored to the SeaDoo configuration.

*NEED* think about generalization of processing with different instruments configuration, usage of \*.con files (similar to ProSoft)

*OPTIONAL*
Could use ECMWF services (climate reanalysis) to add meteorological data to the interface. Like clouds on the selection map ?

# Server

## golem organization 

This shiny app will become quite large. A clear organisation is the key to development and maintenance, golem is here to help in this task. 

* fct_ : files containing the function of the app, they defined the data parsing, tidying and computation logic.
* mod_ : files containing both frontend (`_ui` function) and backend (`_server` function) shiny logic. Inputs, reactive value, observer and consumer are defined here.
* utils_ : files containing helpers functions that doesn't really belong exclusively to this app.
* app_ : files to wrap ui and server, configure and run the app.
* zzz.R : file containing the call to `.onLoad()`

## app_server

The app_server.R wrap up the application server side, it's where all modules are put together and communication between them is defined.
It's also there than global options such as `options(digits.secs = 3)` (to display and save time at the millisecond with 3 digit accuracy) are located.

# SQLite backend

Present the DB schema here

## Save

When an `UUID` already exist and the `Save` button is pressed, the records in the SQLite DB are uptaded. For the `Metadata` table, the following fields are updated:
* `Comment`
* `ProTime`
* `Analyst`
* `Mail`

For the `HOCRL1b` table only the `QC` field is updated, it is uniquely identified by the (UUID, ID) pair. THIS PAIR IS NOT A PRIMARY KEY as it doesn't uniquely identifies the entry of the table. The PRIMARY KEY of the HOCRL1b table is (UUID, ID, SN, Wavelength).
QUESTION: Would it be relevant to separate the `QC` in it's own table ?

When the `Save` button is pressed, feedback is given to the user in the form of a `session$sendCustomMessage`.

Note reagarding diffference bettwen the `WHERE` and the `CASE WHEN` approach for updating SQLite.
Whith 

## Delete

When the delete button is pressed, if the active data point (station) has an `UUID`, it's suppressed in the `Metadata` table. As the FOREIGN KEY constraint is activated in the DB schema (`PRAGMA foreign_keys=ON`) and `HOCRL1b` and `HOCRL2` are child table build with the `ON DELETE CASCADE` constraints, matching record are automatically deleted.
